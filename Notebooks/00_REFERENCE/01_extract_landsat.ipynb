{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c3f57-77b1-46a7-b27f-a31d348bc51e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Planetary Computer tools for STAC API access and authentication\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from odc.stac import stac_load\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a24440",
   "metadata": {},
   "outputs": [],
   "source": "catalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=pc.sign_inplace,\n)\n\ndef create_bbox(lat, lon, patch_size=64, pixel_resolution=30):\n    import math\n    half_width_m = (patch_size * pixel_resolution) / 2\n    lat_buffer = half_width_m / 111320\n    lon_buffer = half_width_m / (111320 * math.cos(math.radians(lat)))\n    return [\n        lon - lon_buffer,\n        lat - lat_buffer,\n        lon + lon_buffer,\n        lat + lat_buffer\n    ]\n\ndef extract_landsat(row, catalog, patch_size=64, pixel_resolution=30, max_cloud_cover=10, time_window=30):\n    \"\"\"\n    Extract a full Landsat patch as an xr.Dataset for a single sample.\n    Returns the raw Dataset with all bands at full spatial resolution, or None.\n    \"\"\"\n    lat = row['Latitude']\n    lon = row['Longitude']\n    sample_date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n\n    bbox = create_bbox(lat=lat, lon=lon, patch_size=patch_size, pixel_resolution=pixel_resolution)\n\n    start_date = sample_date - timedelta(days=time_window)\n    end_date = sample_date + timedelta(days=time_window)\n    datetime_range = f\"{start_date.isoformat()}/{end_date.isoformat()}\"\n\n    search = catalog.search(\n        collections=[\"landsat-c2-l2\"],\n        bbox=bbox,\n        datetime=datetime_range,\n        query={\"eo:cloud_cover\": {\"lt\": max_cloud_cover}},\n    )\n\n    items = search.item_collection()\n    if not items:\n        return None\n\n    try:\n        sample_date_utc = sample_date.tz_localize(\"UTC\") if sample_date.tzinfo is None else sample_date.tz_convert(\"UTC\")\n\n        items = sorted(\n            items,\n            key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n        )\n        selected_item = pc.sign(items[0])\n\n        ds = stac_load([selected_item], bbox=bbox).isel(time=0)\n\n        # Attach metadata\n        ds.attrs['sample_lat'] = lat\n        ds.attrs['sample_lon'] = lon\n        ds.attrs['sample_date'] = str(sample_date.date())\n        ds.attrs['scene_datetime'] = items[0].properties[\"datetime\"]\n\n        return ds\n\n    except Exception as e:\n        print(f\"  Error for ({lat}, {lon}): {e}\")\n        return None\n\n\ndef pad_patch(ds, patch_size=64):\n    \"\"\"Pad or crop all bands in a Dataset to (patch_size, patch_size).\"\"\"\n    result = {}\n    for var in ds.data_vars:\n        arr = ds[var].values.astype(\"float32\")\n        h, w = arr.shape\n        padded = np.full((patch_size, patch_size), np.nan, dtype=\"float32\")\n        padded[:min(h, patch_size), :min(w, patch_size)] = arr[:min(h, patch_size), :min(w, patch_size)]\n        result[var] = padded\n    return result\n\n\ndef save_batch(patches, metadata, batch_idx, output_dir, patch_size=64):\n    \"\"\"\n    Combine a list of extracted patches into a single .nc file.\n    \n    Saved Dataset has dims (sample, y, x) with metadata as coordinates.\n    \"\"\"\n    if not patches:\n        return\n\n    # Pad all patches to uniform size\n    padded = [pad_patch(p, patch_size) for p in patches]\n    band_names = list(padded[0].keys())\n\n    # Stack into (n_samples, patch_size, patch_size) per band\n    batch_ds = xr.Dataset({\n        band: ([\"sample\", \"y\", \"x\"], np.stack([p[band] for p in padded]))\n        for band in band_names\n    })\n\n    # Attach metadata as coordinates on the sample dimension\n    batch_ds.coords[\"lat\"] = (\"sample\", [m[\"lat\"] for m in metadata])\n    batch_ds.coords[\"lon\"] = (\"sample\", [m[\"lon\"] for m in metadata])\n    batch_ds.coords[\"sample_date\"] = (\"sample\", [m[\"sample_date\"] for m in metadata])\n    batch_ds.coords[\"scene_datetime\"] = (\"sample\", [m[\"scene_datetime\"] for m in metadata])\n    batch_ds.coords[\"original_index\"] = (\"sample\", [m[\"original_index\"] for m in metadata])\n\n    path = os.path.join(output_dir, f\"batch_{batch_idx:03d}.nc\")\n    batch_ds.to_netcdf(path)\n    print(f\"  Saved {path} ({len(patches)} patches)\")"
  },
  {
   "cell_type": "code",
   "id": "r82ltp3koum",
   "source": "import xarray as xr\n\nDATA_DIR = os.path.join(\"..\", \"..\", \"Data\")\nPATCH_SIZE = 64\nBATCH_SIZE = 100\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\nval_df = pd.read_csv(os.path.join(DATA_DIR, \"validation.csv\"))\n\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\ntrain_df.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "783jtt4tflt",
   "source": "### Extract Training Patches\nProcesses in batches of 100. Each batch is saved as a `.nc` file immediately, so progress is preserved if the run is interrupted. Already-saved batches are skipped on re-run.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uqv37j0zk5n",
   "source": "train_output_dir = os.path.join(DATA_DIR, \"landsat_patches\", \"train\")\nos.makedirs(train_output_dir, exist_ok=True)\n\nn_batches = (len(train_df) + BATCH_SIZE - 1) // BATCH_SIZE\nfailed_indices = []\n\nfor batch_idx in range(n_batches):\n    batch_path = os.path.join(train_output_dir, f\"batch_{batch_idx:03d}.nc\")\n\n    # Skip batches that are already saved\n    if os.path.exists(batch_path):\n        print(f\"Batch {batch_idx:03d} already exists, skipping.\")\n        continue\n\n    start = batch_idx * BATCH_SIZE\n    end = min(start + BATCH_SIZE, len(train_df))\n    batch_df = train_df.iloc[start:end]\n\n    patches = []\n    metadata = []\n\n    print(f\"Batch {batch_idx:03d} ({start}-{end-1}):\")\n    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"  Extracting\"):\n        ds = extract_landsat(row, catalog, patch_size=PATCH_SIZE)\n        if ds is not None:\n            patches.append(ds)\n            metadata.append({\n                \"lat\": ds.attrs[\"sample_lat\"],\n                \"lon\": ds.attrs[\"sample_lon\"],\n                \"sample_date\": ds.attrs[\"sample_date\"],\n                \"scene_datetime\": ds.attrs[\"scene_datetime\"],\n                \"original_index\": idx,\n            })\n        else:\n            failed_indices.append(idx)\n\n    save_batch(patches, metadata, batch_idx, train_output_dir, patch_size=PATCH_SIZE)\n\nprint(f\"\\nDone. {len(failed_indices)} failed samples: {failed_indices[:20]}{'...' if len(failed_indices) > 20 else ''}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ixbv3neaoda",
   "source": "### Extract Validation Patches",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "33bxn60vp4g",
   "source": "val_output_dir = os.path.join(DATA_DIR, \"landsat_patches\", \"validation\")\nos.makedirs(val_output_dir, exist_ok=True)\n\nn_batches_val = (len(val_df) + BATCH_SIZE - 1) // BATCH_SIZE\nfailed_indices_val = []\n\nfor batch_idx in range(n_batches_val):\n    batch_path = os.path.join(val_output_dir, f\"batch_{batch_idx:03d}.nc\")\n\n    if os.path.exists(batch_path):\n        print(f\"Batch {batch_idx:03d} already exists, skipping.\")\n        continue\n\n    start = batch_idx * BATCH_SIZE\n    end = min(start + BATCH_SIZE, len(val_df))\n    batch_df = val_df.iloc[start:end]\n\n    patches = []\n    metadata = []\n\n    print(f\"Batch {batch_idx:03d} ({start}-{end-1}):\")\n    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"  Extracting\"):\n        ds = extract_landsat(row, catalog, patch_size=PATCH_SIZE)\n        if ds is not None:\n            patches.append(ds)\n            metadata.append({\n                \"lat\": ds.attrs[\"sample_lat\"],\n                \"lon\": ds.attrs[\"sample_lon\"],\n                \"sample_date\": ds.attrs[\"sample_date\"],\n                \"scene_datetime\": ds.attrs[\"scene_datetime\"],\n                \"original_index\": idx,\n            })\n        else:\n            failed_indices_val.append(idx)\n\n    save_batch(patches, metadata, batch_idx, val_output_dir, patch_size=PATCH_SIZE)\n\nprint(f\"\\nDone. {len(failed_indices_val)} failed samples: {failed_indices_val[:20]}{'...' if len(failed_indices_val) > 20 else ''}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vohezar9k9p",
   "source": "### Verify Saved Batches\nLoad a batch back to confirm structure and metadata.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uhqu2sg90v",
   "source": "# Load first training batch and inspect\nsample_batch = xr.open_dataset(os.path.join(train_output_dir, \"batch_000.nc\"))\nprint(\"Dims:\", dict(sample_batch.dims))\nprint(\"Bands:\", list(sample_batch.data_vars))\nprint(\"Coords:\", list(sample_batch.coords))\nprint(f\"\\nSample 0: lat={float(sample_batch.lat[0])}, lon={float(sample_batch.lon[0])}, date={str(sample_batch.sample_date[0].values)}\")\nprint(f\"Patch shape per band: ({sample_batch.dims['y']}, {sample_batch.dims['x']})\")\nsample_batch",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Notebook",
   "name": "jupyter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}