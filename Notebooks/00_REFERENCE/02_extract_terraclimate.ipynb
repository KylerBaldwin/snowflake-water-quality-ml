{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_terraclimate_dataset():\n",
    "    \"\"\"Open the full TerraClimate Zarr store from Planetary Computer.\"\"\"\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=pc.sign_inplace,\n",
    "    )\n",
    "    collection = catalog.get_collection(\"terraclimate\")\n",
    "    asset = collection.assets[\"zarr-abfs\"]\n",
    "\n",
    "    if \"xarray:storage_options\" in asset.extra_fields:\n",
    "        ds = xr.open_zarr(\n",
    "            asset.href,\n",
    "            storage_options=asset.extra_fields[\"xarray:storage_options\"],\n",
    "            consolidated=True,\n",
    "        )\n",
    "    else:\n",
    "        ds = xr.open_dataset(\n",
    "            asset.href,\n",
    "            **asset.extra_fields[\"xarray:open_kwargs\"],\n",
    "        )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def extract_terraclimate(row, ds, spatial_buffer=0.125, months_back=12):\n",
    "    \"\"\"\n",
    "    Extract a TerraClimate spatiotemporal patch for a single sample.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row with Latitude, Longitude, Sample Date\n",
    "        ds: Full TerraClimate xr.Dataset (opened once)\n",
    "        spatial_buffer: Degrees around the point (~0.042 deg per pixel at ~4km)\n",
    "                        Default 0.125 gives roughly a 7x7 pixel patch\n",
    "        months_back: Number of months before the sample date to include.\n",
    "                     Default 12 captures a full seasonal cycle.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset with dims (time, lat, lon) and all TerraClimate variables, or None.\n",
    "    \"\"\"\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    sample_date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "    if pd.isna(sample_date):\n",
    "        return None\n",
    "\n",
    "    # Temporal window: months_back months before sample date up to sample date\n",
    "    end_date = sample_date\n",
    "    start_date = sample_date - pd.DateOffset(months=months_back)\n",
    "\n",
    "    # Spatial window\n",
    "    lat_min, lat_max = lat - spatial_buffer, lat + spatial_buffer\n",
    "    lon_min, lon_max = lon - spatial_buffer, lon + spatial_buffer\n",
    "\n",
    "    try:\n",
    "        patch = ds.sel(\n",
    "            time=slice(str(start_date.date()), str(end_date.date())),\n",
    "            lat=slice(lat_max, lat_min),   # lat is descending in TerraClimate\n",
    "            lon=slice(lon_min, lon_max),\n",
    "        )\n",
    "\n",
    "        if patch.dims['time'] == 0 or patch.dims['lat'] == 0 or patch.dims['lon'] == 0:\n",
    "            return None\n",
    "\n",
    "        # Load into memory (small slice)\n",
    "        patch = patch.load()\n",
    "\n",
    "        # Attach metadata\n",
    "        patch.attrs['sample_lat'] = lat\n",
    "        patch.attrs['sample_lon'] = lon\n",
    "        patch.attrs['sample_date'] = str(sample_date.date())\n",
    "\n",
    "        return patch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error for ({lat}, {lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_batch(patches, metadata, batch_idx, output_dir, target_spatial=7, target_time=13):\n",
    "    \"\"\"\n",
    "    Combine patches into a single .nc file.\n",
    "\n",
    "    Pads/crops spatial dims to (target_spatial, target_spatial)\n",
    "    and time dim to target_time so all batches are uniform.\n",
    "\n",
    "    Saved Dataset has dims (sample, time, lat, lon).\n",
    "    \"\"\"\n",
    "    if not patches:\n",
    "        return\n",
    "\n",
    "    band_names = [v for v in patches[0].data_vars]\n",
    "\n",
    "    # Pad each patch to uniform dims\n",
    "    padded_bands = {band: [] for band in band_names}\n",
    "\n",
    "    for p in patches:\n",
    "        for band in band_names:\n",
    "            arr = p[band].values.astype(\"float32\")\n",
    "            t, h, w = arr.shape\n",
    "\n",
    "            out = np.full((target_time, target_spatial, target_spatial), np.nan, dtype=\"float32\")\n",
    "            out[:min(t, target_time), :min(h, target_spatial), :min(w, target_spatial)] = \\\n",
    "                arr[:min(t, target_time), :min(h, target_spatial), :min(w, target_spatial)]\n",
    "            padded_bands[band].append(out)\n",
    "\n",
    "    # Stack into (n_samples, time, lat, lon) per band\n",
    "    batch_ds = xr.Dataset({\n",
    "        band: ([\"sample\", \"time\", \"y\", \"x\"], np.stack(padded_bands[band]))\n",
    "        for band in band_names\n",
    "    })\n",
    "\n",
    "    # Attach metadata as coordinates\n",
    "    batch_ds.coords[\"lat\"] = (\"sample\", [m[\"lat\"] for m in metadata])\n",
    "    batch_ds.coords[\"lon\"] = (\"sample\", [m[\"lon\"] for m in metadata])\n",
    "    batch_ds.coords[\"sample_date\"] = (\"sample\", [m[\"sample_date\"] for m in metadata])\n",
    "    batch_ds.coords[\"original_index\"] = (\"sample\", [m[\"original_index\"] for m in metadata])\n",
    "\n",
    "    path = os.path.join(output_dir, f\"batch_{batch_idx:03d}.nc\")\n",
    "    batch_ds.to_netcdf(path)\n",
    "    print(f\"  Saved {path} ({len(patches)} patches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"..\", \"Data\")\n",
    "BATCH_SIZE = 100\n",
    "SPATIAL_BUFFER = 0.125   # ~7 pixels at ~4km\n",
    "MONTHS_BACK = 12         # full seasonal cycle\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(DATA_DIR, \"validation.csv\"))\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Open TerraClimate once (lazy Zarr, no download yet)\n",
    "ds = load_terraclimate_dataset()\n",
    "print(f\"\\nTerraClimate variables ({len(ds.data_vars)}): {list(ds.data_vars)}\")\n",
    "print(f\"Time range: {str(ds.time.values[0])[:10]} to {str(ds.time.values[-1])[:10]}\")\n",
    "print(f\"Spatial resolution: ~{abs(float(ds.lat[1] - ds.lat[0])):.4f} deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "### Extract Training Patches\n",
    "Processes in batches of 100. Each batch is saved as a `.nc` file immediately, so progress is preserved if interrupted. Already-saved batches are skipped on re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_dir = os.path.join(DATA_DIR, \"terraclimate_patches\", \"train\")\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "\n",
    "n_batches = (len(train_df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "failed_indices = []\n",
    "\n",
    "for batch_idx in range(n_batches):\n",
    "    batch_path = os.path.join(train_output_dir, f\"batch_{batch_idx:03d}.nc\")\n",
    "\n",
    "    if os.path.exists(batch_path):\n",
    "        print(f\"Batch {batch_idx:03d} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    start = batch_idx * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, len(train_df))\n",
    "    batch_df = train_df.iloc[start:end]\n",
    "\n",
    "    patches = []\n",
    "    metadata = []\n",
    "\n",
    "    print(f\"Batch {batch_idx:03d} ({start}-{end-1}):\")\n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=\"  Extracting\"):\n",
    "        patch = extract_terraclimate(row, ds, spatial_buffer=SPATIAL_BUFFER, months_back=MONTHS_BACK)\n",
    "        if patch is not None:\n",
    "            patches.append(patch)\n",
    "            metadata.append({\n",
    "                \"lat\": patch.attrs[\"sample_lat\"],\n",
    "                \"lon\": patch.attrs[\"sample_lon\"],\n",
    "                \"sample_date\": patch.attrs[\"sample_date\"],\n",
    "                \"original_index\": idx,\n",
    "            })\n",
    "        else:\n",
    "            failed_indices.append(idx)\n",
    "\n",
    "    save_batch(patches, metadata, batch_idx, train_output_dir)\n",
    "\n",
    "print(f\"\\nDone. {len(failed_indices)} failed samples: {failed_indices[:20]}{'...' if len(failed_indices) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-header",
   "metadata": {},
   "source": [
    "### Extract Validation Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "val-extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_output_dir = os.path.join(DATA_DIR, \"terraclimate_patches\", \"validation\")\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "n_batches_val = (len(val_df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "failed_indices_val = []\n",
    "\n",
    "for batch_idx in range(n_batches_val):\n",
    "    batch_path = os.path.join(val_output_dir, f\"batch_{batch_idx:03d}.nc\")\n",
    "\n",
    "    if os.path.exists(batch_path):\n",
    "        print(f\"Batch {batch_idx:03d} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    start = batch_idx * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, len(val_df))\n",
    "    batch_df = val_df.iloc[start:end]\n",
    "\n",
    "    patches = []\n",
    "    metadata = []\n",
    "\n",
    "    print(f\"Batch {batch_idx:03d} ({start}-{end-1}):\")\n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=\"  Extracting\"):\n",
    "        patch = extract_terraclimate(row, ds, spatial_buffer=SPATIAL_BUFFER, months_back=MONTHS_BACK)\n",
    "        if patch is not None:\n",
    "            patches.append(patch)\n",
    "            metadata.append({\n",
    "                \"lat\": patch.attrs[\"sample_lat\"],\n",
    "                \"lon\": patch.attrs[\"sample_lon\"],\n",
    "                \"sample_date\": patch.attrs[\"sample_date\"],\n",
    "                \"original_index\": idx,\n",
    "            })\n",
    "        else:\n",
    "            failed_indices_val.append(idx)\n",
    "\n",
    "    save_batch(patches, metadata, batch_idx, val_output_dir)\n",
    "\n",
    "print(f\"\\nDone. {len(failed_indices_val)} failed samples: {failed_indices_val[:20]}{'...' if len(failed_indices_val) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "### Verify Saved Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = xr.open_dataset(os.path.join(train_output_dir, \"batch_000.nc\"))\n",
    "print(\"Dims:\", dict(sample_batch.dims))\n",
    "print(\"Variables:\", list(sample_batch.data_vars))\n",
    "print(\"Coords:\", list(sample_batch.coords))\n",
    "print(f\"\\nSample 0: lat={float(sample_batch.lat[0])}, lon={float(sample_batch.lon[0])}, date={str(sample_batch.sample_date[0].values)}\")\n",
    "sample_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}